# Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification

This repository contains the **implementation, dataset, and experiments** for the paper:  
**â€œExplanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification.â€**

We explore two state-of-the-art **Explanatory Interactive Learning (XIL)** strategies â€” **CAIPI** and **Right for the Right Reasons (RRR)** â€” and propose a **novel hybrid approach** to guide visual classifiers toward more relevant features, reducing bias and improving fairness in gender classification tasks.

---

## ğŸš€ Features
- Implementation of **CAIPI** and **RRR** XIL strategies  
- Novel **Hybrid XIL approach** combining strengths of both methods  
- Evaluation of **fairness, transparency, and interpretability** in visual gender classification  
- Comparisons using **GradCAM** and **Bounded Logit Attention (BLA)** for model explanations  
- Experiments showing how XIL can reduce **bias** (e.g., balancing misclassification rates between male and female predictions)  

---

## ğŸ“¦ Installation
Clone the repository:
```bash
git clone https://github.com/yourusername/xil-bias-gender-classification.git
cd xil-bias-gender-classification

pip install -r requirements.txt
```

---

## ğŸ“Š Experiments
We evaluate models on:
- Baseline classifiers without XIL
- **CAIPI** with varying numbers of augmentations
- **RRR** with explanation-based regularization
- Hybrid combining **CAIPI** and **RRR**
- 2 explainability methods **GradCAM** and **Bounded Logit Attention (BLA)**
- 2 sampling strategies **Uncertainty** sampling **High Confidence** sampling

---

## ğŸ“‚ Dataset
The dataset used in this study is a **subset of the [MS COCO dataset](https://cocodataset.org)** that contains images of persons.  
Since MS COCO does not natively provide annotations for gender or sex, a **manual data selection process** was carried out to construct a **binary gender classification dataset**.  

âš ï¸ Note: We are aware that binary gender represents an over-simplification. To limit task complexity and simplify result interpretation, the scope of this study was restricted to binary classification.

---

## ğŸ“– Citation
If you use this work, please cite:
```bibtex
  @article{tbd
  }
```

